{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13c35bf5-ce90-43ca-a9b0-6126e204c8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[classic-control] in /opt/conda/lib/python3.11/site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.11/site-packages (from gymnasium[classic-control]) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from gymnasium[classic-control]) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.11/site-packages (from gymnasium[classic-control]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.11/site-packages (from gymnasium[classic-control]) (0.0.4)\n",
      "Requirement already satisfied: pygame>=2.1.3 in /opt/conda/lib/python3.11/site-packages (from gymnasium[classic-control]) (2.6.0)\n",
      "Requirement already satisfied: renderlab in /opt/conda/lib/python3.11/site-packages (0.1.20230421184216)\n",
      "Requirement already satisfied: moviepy in /opt/conda/lib/python3.11/site-packages (from renderlab) (1.0.3)\n",
      "Requirement already satisfied: gymnasium in /opt/conda/lib/python3.11/site-packages (from renderlab) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.11/site-packages (from gymnasium->renderlab) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from gymnasium->renderlab) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.11/site-packages (from gymnasium->renderlab) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.11/site-packages (from gymnasium->renderlab) (0.0.4)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in /opt/conda/lib/python3.11/site-packages (from moviepy->renderlab) (4.4.2)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /opt/conda/lib/python3.11/site-packages (from moviepy->renderlab) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in /opt/conda/lib/python3.11/site-packages (from moviepy->renderlab) (2.32.3)\n",
      "Requirement already satisfied: proglog<=1.0.0 in /opt/conda/lib/python3.11/site-packages (from moviepy->renderlab) (0.1.10)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /opt/conda/lib/python3.11/site-packages (from moviepy->renderlab) (2.34.2)\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from moviepy->renderlab) (0.5.1)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /opt/conda/lib/python3.11/site-packages (from imageio<3.0,>=2.5->moviepy->renderlab) (10.4.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from imageio-ffmpeg>=0.2.0->moviepy->renderlab) (70.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3.0,>=2.8.1->moviepy->renderlab) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3.0,>=2.8.1->moviepy->renderlab) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3.0,>=2.8.1->moviepy->renderlab) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3.0,>=2.8.1->moviepy->renderlab) (2024.6.2)\n",
      "Requirement already satisfied: opencv-python in /opt/conda/lib/python3.11/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.11/site-packages (from opencv-python) (1.26.4)\n",
      "Requirement already satisfied: gymnasium[box2d] in /opt/conda/lib/python3.11/site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.11/site-packages (from gymnasium[box2d]) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from gymnasium[box2d]) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.11/site-packages (from gymnasium[box2d]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.11/site-packages (from gymnasium[box2d]) (0.0.4)\n",
      "Requirement already satisfied: box2d-py==2.3.5 in /opt/conda/lib/python3.11/site-packages (from gymnasium[box2d]) (2.3.5)\n",
      "Requirement already satisfied: pygame>=2.1.3 in /opt/conda/lib/python3.11/site-packages (from gymnasium[box2d]) (2.6.0)\n",
      "Requirement already satisfied: swig==4.* in /opt/conda/lib/python3.11/site-packages (from gymnasium[box2d]) (4.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install gymnasium[classic-control]\n",
    "!pip3 install renderlab\n",
    "!pip3 install opencv-python\n",
    "!pip3 install gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e19f19ba-9c96-4fe8-a19c-37c589c0aa81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n",
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n",
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import renderlab as rl\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de486884-0539-4210-bbb4-402bd81b5c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_factory():\n",
    "    return np.zeros(env.action_space.n)\n",
    "class Agent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float,\n",
    "        initial_epsilon: float,\n",
    "        epsilon_decay: float,\n",
    "        final_epsilon: float,\n",
    "        discount_factor: float = 0.95,\n",
    "    ):\n",
    "        self.q_values = defaultdict(zero_factory)\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "\n",
    "        self.training_error = []\n",
    "    def get_action(self, obs: tuple[int, int]) -> int:\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            return int(np.argmax(self.q_values[obs]))\n",
    "    def update(\n",
    "        self,\n",
    "        obs: tuple[int, int],\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        terminated: bool,\n",
    "        next_obs: tuple[int, int],\n",
    "    ):\n",
    "        future_q_value = (not terminated) * np.max(self.q_values[next_obs])\n",
    "        temporal_difference = (\n",
    "            reward + self.discount_factor * future_q_value - self.q_values[obs][action]\n",
    "        )\n",
    "        self.q_values[obs][action] = (\n",
    "            self.q_values[obs][action] + self.lr * temporal_difference\n",
    "        )\n",
    "        self.training_error.append(temporal_difference)\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - epsilon_decay)\n",
    "def save(obj, path):\n",
    "    with open(path, 'bw') as f:\n",
    "        pickle.dump(obj, f)\n",
    "def load(path):\n",
    "    with open(path, 'br') as f:\n",
    "        return pickle.load(f)\n",
    "def discretize(obs, decimal):\n",
    "    return np.round(obs, decimals = decimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "698f12d4-a364-46f6-919e-1122b0d4f6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|██████████████▍                                                                                                                                 | 9999/100000 [14:45<2:21:38, 10.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rewards over the last 10000 iterations -166.80437942905425\n",
      "Max rewards over the last 10000 iterations 100.43367767333984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████████████████▌                                                                                                                  | 20000/100000 [33:07<2:35:06,  8.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rewards over the last 10000 iterations -158.85641777648925\n",
      "Max rewards over the last 10000 iterations 132.18368530273438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██████████████████████████████████████████▉                                                                                                    | 30000/100000 [56:29<2:33:26,  7.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rewards over the last 10000 iterations -171.90407308330535\n",
      "Max rewards over the last 10000 iterations 107.49858856201172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████████████████████████████████████████▍                                                                                    | 40000/100000 [1:25:39<3:22:12,  4.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rewards over the last 10000 iterations -185.94770835456848\n",
      "Max rewards over the last 10000 iterations 282.68231201171875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████████████████████████████████████████▍                                                                                    | 40000/100000 [1:29:22<2:14:04,  7.46it/s]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage rewards over the last \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m iterations \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39maverage(rewards)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMax rewards over the last \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m iterations \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmax(rewards)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m         \u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./checkpoint\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m save(agent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./checkpoint\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     46\u001b[0m env \u001b[38;5;241m=\u001b[39m rl\u001b[38;5;241m.\u001b[39mRenderFrame(env, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./out\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 48\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, path)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave\u001b[39m(obj, path):\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 48\u001b[0m         \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\n",
    "    \"LunarLander-v2\", render_mode=\"rgb_array\",\n",
    "    continuous = False,\n",
    "    gravity = -10.0,\n",
    "    enable_wind = False,\n",
    "    wind_power = 15.0,\n",
    "    turbulence_power = 1.5,\n",
    ")\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "done = False\n",
    "learning_rate = 0.01\n",
    "n_episodes = 100000\n",
    "start_epsilon = 1.0\n",
    "epsilon_decay = start_epsilon / (n_episodes / 2)\n",
    "final_epsilon = 0.1\n",
    "num_chunks = 10\n",
    "chunk_length = int(n_episodes / num_chunks)\n",
    "rewards = np.zeros(chunk_length)\n",
    "agent = Agent(\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    final_epsilon=final_epsilon,\n",
    ")\n",
    "#agent = load('./checkpoint')\n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    obs, info = env.reset()\n",
    "    obs = discretize(obs, 1)\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.get_action(tuple(obs))\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        next_obs = discretize(next_obs, 1)\n",
    "        agent.update(tuple(obs), action, reward, terminated, tuple(next_obs))\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "    agent.decay_epsilon()\n",
    "    rewards[episode % chunk_length] = info['episode']['r']\n",
    "    if episode % chunk_length == 0 and episode > 0:\n",
    "        print(f'Average rewards over the last {chunk_length} iterations {np.average(rewards)}')\n",
    "        print(f'Max rewards over the last {chunk_length} iterations {np.max(rewards)}')\n",
    "        save(agent, './checkpoint')\n",
    "    \n",
    "save(agent, './checkpoint')\n",
    "\n",
    "env = rl.RenderFrame(env, './out')\n",
    "for episode in range(7):\n",
    "    obs, info = env.reset()\n",
    "    obs = discretize(obs, 1)\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.get_action(tuple(obs))\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        next_obs = discretize(next_obs, 1)\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "env.play()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782812cd-d53d-4a7c-86ec-e00fe3f2d453",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
